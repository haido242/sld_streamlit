import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase
import av
import numpy as np
import cv2
from collections import deque
from deploy_model import extract_frame_features, model, label_encoder, THRESHOLD
import time
import json
import asyncio

METADATA_PATH = './data/metadata.json'
metadata = {}
with open(METADATA_PATH, 'r', encoding='utf-8') as f:
    metadata = json.load(f)

# -------------------- UI Setup --------------------
st.set_page_config(layout="wide")
# áº¨n footer, menu, header
hide_streamlit_style = """
    <style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stMainBlockContainer {padding: 0;}
    </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

if "labels" not in st.session_state:
    st.session_state.labels = []

if st.button("ðŸ—‘ï¸ XoÃ¡ káº¿t quáº£"):
    st.session_state.labels.clear()

result_placeholder = st.empty()

result_placeholder.success(f"Nháº­n diá»‡n: ")

# -------------------- Video Processor --------------------
class SignLanguageProcessor(VideoProcessorBase):
    def __init__(self):
        # Biáº¿n lÆ°u káº¿t quáº£ má»›i nháº­n diá»‡n
        self.result = None
        self.window = deque(maxlen=30)

        # Biáº¿n tráº¡ng thÃ¡i cho frame trÆ°á»›c
        self.prev_right = self.prev_left = None
        self.prev_right_center = self.prev_left_center = None
        self.prev_right_shoulder_dists = (0.0, 0.0)
        self.prev_left_shoulder_dists = (0.0, 0.0)
        self.prev_shoulder_left = self.prev_shoulder_right = None

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")

        # Kiá»ƒm tra áº£nh há»£p lá»‡ (trÃ¡nh lá»—i MediaPipe)
        if img is None or img.shape[0] == 0 or img.shape[1] == 0:
            return av.VideoFrame.from_ndarray(img, format="bgr24")

        # TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng
        (
            frame_features,
            hand_detected,
            self.prev_right,
            self.prev_left,
            self.prev_right_center,
            self.prev_left_center,
            right_shoulder_dists,
            left_shoulder_dists,
            self.prev_shoulder_left,
            self.prev_shoulder_right,
        ) = extract_frame_features(
            img,
            self.prev_right,
            self.prev_left,
            self.prev_right_center,
            self.prev_left_center,
            self.prev_right_shoulder_dists,
            self.prev_left_shoulder_dists,
            self.prev_shoulder_left,
            self.prev_shoulder_right,
        )

        self.window.append(frame_features)

        if len(self.window) == 30:
            pred = model.predict(np.array(self.window)[np.newaxis, ...], verbose=0)
            confidence = np.max(pred)
            if confidence >= THRESHOLD:
                pred_class = np.argmax(pred, axis=1)
                predicted_label = label_encoder.inverse_transform(pred_class)[0]
                self.result = metadata.get(predicted_label, "KhÃ´ng nháº­n diá»‡n Ä‘Æ°á»£c")
            self.window.clear()

        return av.VideoFrame.from_ndarray(img, format="bgr24")


# -------------------- Giao tiáº¿p vá»›i processor --------------------
ctx = webrtc_streamer(
    key="sign_lang",
    video_processor_factory=SignLanguageProcessor,
    media_stream_constraints={"video": True, "audio": False},
    rtc_configuration={
        "iceServers": [
            {"urls": ["stun:stun.l.google.com:19302"]},
            {
                "urls": [
                    "turn:global.relay.metered.ca:80",
                    "turn:global.relay.metered.ca:443",
                    "turn:global.relay.metered.ca:443?transport=tcp"
                ],
                "username": "openai",
                "credential": "openai123"
            }
        ]
    },
    async_processing=True,
)


# Cáº­p nháº­t giao diá»‡n tá»« káº¿t quáº£ xá»­ lÃ½ video má»—i 1 giÃ¢y
while ctx.state.playing and ctx.video_processor:
    if ctx.video_processor.result:
        st.session_state.labels.append(ctx.video_processor.result)
        result_placeholder.success(f"Nháº­n diá»‡n: {' / '.join(st.session_state.labels)}")
        ctx.video_processor.result = None  # reset Ä‘á»ƒ khÃ´ng láº·p láº¡i
    else:
        # Hiá»ƒn thá»‹ káº¿t quáº£ hiá»‡n táº¡i náº¿u cÃ³
        if st.session_state.labels:
            result_placeholder.success(f"Nháº­n diá»‡n: {' / '.join(st.session_state.labels)}")
    time.sleep(1)

try:
    asyncio.get_running_loop()
except RuntimeError:
    asyncio.set_event_loop(asyncio.new_event_loop())